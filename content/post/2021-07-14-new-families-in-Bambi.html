---
title: "New families in Bambi"
author: "Tomás Capretto"
date: 2021-07-14
output: html_document
description: In this third post about my work during this Google Summer of Code
  I describe two families of models recently added. The first one, is the 
  Student T family, used to make linear regressions more robust. The second, 
  is the Beta family which can be used to model ratings and proportions. 
editor_options:
  chunk_output_type: console
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<hr />
<p>I’m very happy I could contribute with many exciting changes to
<a href="https://bambinos.github.io/bambi">Bambi</a>. Some changes, such as the
reorganization of the default priors and built-in families, are not visible to
the user but make the codebase more modular and easier to read. Other changes,
such as the ones I’m going to describe here, have a direct impact on what you
can do with Bambi.</p>
<p>Today I’ll describe two new built-in families that have been added to Bambi.
The first one, already described in my previous post, is the <code>"t"</code> family. This
can be used to make linear regressions more robust to outliers. The second one the
<code>"beta"</code> family which can be used to model ratings and proportions.</p>
</div>
<div id="setup" class="section level2">
<h2>Setup</h2>
<hr />
<pre class="python"><code>import arviz as az
import numpy as np
import pandas as pd

from bambi import Model, Prior</code></pre>
</div>
<div id="robust-linear-regression-with-the-t-family." class="section level2">
<h2>Robust linear regression with the <code>t</code> family.</h2>
<hr />
<p>A Bayesian robust linear regression looks as follows</p>
<p><span class="math display">\[
y_i \sim \text{StudentT}(\mu_i, \lambda, \nu)
\]</span></p>
<p>where <span class="math inline">\(\mu_i = \beta_0 + \beta_1 x_{1, i} + \cdots + \beta_p x_{p, i}\)</span>, <span class="math inline">\(\lambda\)</span> is
the precision parameter and <span class="math inline">\(\nu\)</span> is the degrees of freedom.</p>
<p>This wouldn’t be a Bayesian model without priors. Bambi uses the following
priors by default:</p>
<p><span class="math display">\[
\begin{array}{c}
\beta_0 \sim \text{Normal}(\mu_{\beta_0}, \sigma_{\beta_0}) \\
\beta_j \sim \text{Normal}(\mu_{\beta_j}, \sigma_{\beta_j})  \\
\lambda \sim \text{HalfCauchy(1)}
\end{array}
\]</span></p>
<p>where the <span class="math inline">\(\mu_{\beta_j}\)</span> and <span class="math inline">\(\sigma_{\beta_j}\)</span> are estimated from the data.
By default, <span class="math inline">\(\nu=2\)</span>, but it is also possible to assign it a probability
distribution (as we’re going to see below).</p>
<p>Before seeing how this new family works, let’s simulate some data. On this
opportunity, we’re using the same dataset than in the <a href="https://tcapretto.netlify.app/post/2021-07-05-robust-linear-regression-with-bambi/">previous post</a>.
This is a toy dataset with one predictor <code>x</code>, one response <code>y</code>,
and some outliers contaminating the beautiful linear relationship between the
variables.</p>
<pre class="python"><code>size = 100
true_intercept = 1
true_slope = 2

x = np.linspace(0, 1, size)
y = true_intercept + true_slope * x + np.random.normal(scale=0.5, size=size)

x_out = np.append(x, [0.1, 0.15, 0.2])
y_out = np.append(y, [8, 6, 9])

data = pd.DataFrame(dict(x = x_out, y = y_out))</code></pre>
<p><img src="/post/2021-07-14-new-families-in-Bambi_files/figure-html/unnamed-chunk-3-1.png" width="95%" style="display: block; margin: auto;" /></p>
<div id="model-specification-and-fit" class="section level3">
<h3>Model specification and fit</h3>
<hr />
<p>Using this new family is extremely easy. It is almost as simple as running a
default normal linear regression. The only difference is that we need to add
the <code>family="t"</code> argument to the <code>Model()</code> instantiation.</p>
<pre class="python"><code>model = Model(&quot;y ~ x&quot;, data, family=&quot;t&quot;)
model</code></pre>
<pre><code>## Formula: y ~ x
## Family name: T
## Link: identity
## Observations: 103
## Priors:
##   Common-level effects
##     Intercept ~ Normal(mu: 0, sigma: 2.5)
##     x ~ Normal(mu: 0.0, sigma: 8.5223)
## 
##   Auxiliary parameters
##     lam ~ HalfCauchy(beta: 1)
##     nu ~ 2</code></pre>
<p>The output above shows information about the family being used and the parameters
for the default priors. Next, we just do <code>model.fit()</code> to run the sampler.</p>
<pre class="python"><code>idata = model.fit()</code></pre>
</div>
<div id="use-custom-priors" class="section level3">
<h3>Use custom priors</h3>
<hr />
<p>Let’s say we are not happy with having a fixed value for the degrees of freedom
and we want to assign it a prior distribution. Is that a problem? Of course not!</p>
<pre class="python"><code># Use a Gamma prior for the degrees of freedom
model = Model(&quot;y ~ x&quot;, data, family=&quot;t&quot;)
model.set_priors({&quot;nu&quot;: Prior(&quot;Gamma&quot;, alpha=3, beta=1)})
model</code></pre>
<pre><code>## Formula: y ~ x
## Family name: T
## Link: identity
## Observations: 103
## Priors:
##   Common-level effects
##     Intercept ~ Normal(mu: 0, sigma: 2.5)
##     x ~ Normal(mu: 0.0, sigma: 8.5223)
## 
##   Auxiliary parameters
##     lam ~ HalfCauchy(beta: 1)
##     nu ~ Gamma(alpha: 3, beta: 1)</code></pre>
<p>And hit the inference button</p>
<pre class="python"><code>idata = model.fit()</code></pre>
<pre><code>## █
## Auto-assigning NUTS sampler...
## Initializing NUTS using jitter+adapt_diag...
## Multiprocess sampling (2 chains in 2 jobs)
## NUTS: [y_nu, y_lam, x, Intercept]
## Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds.</code></pre>
</div>
<div id="explore-results" class="section level3">
<h3>Explore results</h3>
<hr />
<p>First of all we can see the marginal posteriors for the parameters in the model
and their respective traces</p>
<p><img src="/post/2021-07-14-new-families-in-Bambi_files/figure-html/post-t-plot-1.png" width="95%" style="display: block; margin: auto;" /></p>
<p>And it is also good to explore the posterior distribution of regression lines</p>
<p><img src="/post/2021-07-14-new-families-in-Bambi_files/figure-html/post-t-plot-2-1.png" width="95%" style="display: block; margin: auto;" /></p>
<p>where the line in black is the true regression line.</p>
</div>
</div>
<div id="beta-regression-with-the-beta-family." class="section level2">
<h2>Beta regression with the <code>beta</code> family.</h2>
<hr />
<p>Beta regression is useful to model response variables that have values within
the <span class="math inline">\((0, 1)\)</span> interval. This type of regression is based on the assumption that
the conditional distribution of the response variable follows a Beta distribution
with its mean related to a set of regressors through a linear predictor with
unknown coefficients and a link function.</p>
<p>The beta regression model is based on an alternative parameterization of the
beta density in terms of the mean <span class="math inline">\(\mu\)</span> and a precision parameter <span class="math inline">\(\kappa\)</span>.</p>
<p><span class="math display">\[
\begin{array}{lr}
\displaystyle f(y | \mu, \kappa) = 
  \frac{\Gamma(\kappa)}{\Gamma(\mu\kappa)\Gamma((1-\mu)\kappa)} 
  y^{\mu\kappa -1}
  y^{(1 - \mu)\kappa -1}, &amp; 0 &lt; y &lt; 1
\end{array}
\]</span></p>
<p>with <span class="math inline">\(0 &lt; \mu &lt; 1\)</span> and <span class="math inline">\(\kappa &gt; 0\)</span>.</p>
<p>If we use the same notation than for the robust linear regression,
the beta regression model is defined as</p>
<p><span class="math display">\[
y_i \sim \text{Beta}(g^{-1}(\mu_i), \kappa)
\]</span></p>
<p>where <span class="math inline">\(\mu_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i}\)</span>, <span class="math inline">\(\kappa\)</span> is
the precision parameter and <span class="math inline">\(g\)</span> is a twice differentiable, strictly increasing,
link function.</p>
<p>Bambi uses again the following priors by default:</p>
<p><span class="math display">\[
\begin{array}{c}
\beta_0 \sim \text{Normal}(\mu_{\beta_0}, \sigma_{\beta_0}) \\
\beta_j \sim \text{Normal}(\mu_{\beta_j}, \sigma_{\beta_j})  \\
\kappa \sim \text{HalfCauchy(1)}
\end{array}
\]</span></p>
<p>where the <span class="math inline">\(\mu_{\beta_j}\)</span> and <span class="math inline">\(\sigma_{\beta_j}\)</span> are estimated from the data.
By default, <span class="math inline">\(g\)</span> is the logit function. Other options available are the identity,
the probit, and the cloglog link functions.</p>
<p>It’s possible to resume all of this in a very simplistic way by seeing that
the beta regression as a very close relative of the GLM family.
This model presents all the characteristics of GLMs, with the exception that the
beta distribution doesn’t belong to the exponential family.</p>
<div id="model-specification-and-fit-1" class="section level3">
<h3>Model specification and fit</h3>
<hr />
<p>Here we are going to use the <code>GasolineYield</code> dataset from the <code>betareg</code> R package.
This dataset is about the proportion of crude oil converted to gasoline.
The response variable is the proportion of crude oil after distillation and
fractionation. In this example, we use the temperature at which gasoline has
vaporized in Fahrenheit degrees (<code>"temp"</code>) and a factor that indicates ten unique
combinations of gravity, pressure and temperature (<code>"batch"</code>).</p>
<p>The following is just a re-ordering of the categories in the <code>"batch"</code> variable
so it matches the original contrasts used in the <code>betareg</code> package.</p>
<pre class="python"><code>data = r.GasolineYield
data[&quot;batch&quot;] = pd.Categorical(
  data[&quot;batch&quot;], 
  [&quot;10&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;9&quot;], 
  ordered=True
)</code></pre>
<p>Next, we define the model. The only difference is that we indicate
<code>family="beta"</code>. Bambi handles all the rest for us.</p>
<pre class="python"><code># Note this model does not include an intercept
model = Model(&quot;yield ~ 0 + temp + batch&quot;, data, family=&quot;beta&quot;)
model</code></pre>
<pre><code>## Formula: yield ~ 0 + temp + batch
## Family name: Beta
## Link: logit
## Observations: 32
## Priors:
##   Common-level effects
##     temp ~ Normal(mu: 0.0, sigma: 0.0364)
##     batch ~ Normal(mu: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], sigma: [ 8.5769  7.5593  8.5769  8.5769  7.5593  8.5769  8.5769  7.5593  8.5769
##  10.328 ])
## 
##   Auxiliary parameters
##     kappa ~ HalfCauchy(beta: 1)</code></pre>
<p>And <code>model.fit()</code> is all we need to ask the sampler to start running.</p>
<pre class="python"><code>idata = model.fit(target_accept=0.9)</code></pre>
<pre><code>## █
## Auto-assigning NUTS sampler...
## Initializing NUTS using jitter+adapt_diag...
## Multiprocess sampling (2 chains in 2 jobs)
## NUTS: [yield_kappa, batch, temp]
## Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 10 seconds.
## The estimated number of effective samples is smaller than 200 for some parameters.</code></pre>
</div>
<div id="explore-results-1" class="section level3">
<h3>Explore results</h3>
<hr />
<p>Once we got the posterior, we explore it. This time we’re going to plot
highest density intervals for the marginal posteriors corresponding to the
parameters in the model.</p>
<p><img src="/post/2021-07-14-new-families-in-Bambi_files/figure-html/posterior-model-beta-plot-1.png" width="100%" style="display: block; margin: auto;" /></p>
</div>
</div>
