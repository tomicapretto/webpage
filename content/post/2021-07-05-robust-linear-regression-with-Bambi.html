---
title: "Robust linear regression in Bambi"
author: "Tomás Capretto"
date: 2021-07-05
output: html_document
description: Second post about this Google Summer of Code season. Today I show
  some of the problems associated with outliers in linear regression and demonstrate
  how one can implement a robust linear regression in Bambi.
editor_options: 
  chunk_output_type: console
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>The next thing in my TODO list for this Google Summer of Code season with
<a href="https://numfocus.org/">NumFOCUS</a> is to add new families of models to
<a href="https://bambinos.github.io/bambi">Bambi</a>. This is still a WIP but I wanted to
show you how to build a robust linear regression model using the <code>Family</code> class
in Bambi.</p>
<div id="setup" class="section level2">
<h2>Setup</h2>
<pre class="r"><code>library(ggplot2)
library(reticulate)
use_condaenv(&quot;rbambi&quot;, required = TRUE)

BLUE = &quot;#003f5c&quot;
PURPLE = &quot;#7a5195&quot;
PINK = &quot;#ef5675&quot;</code></pre>
<pre class="python"><code>import numpy as np
import pandas as pd

from bambi import Model, Family, Prior
from scipy import stats</code></pre>
</div>
<div id="what-do-we-mean-with-robust" class="section level2">
<h2>What do we mean with robust?</h2>
<p>Before showing how to build a robust regression with Bambi we need to be clear
about what we mean when we say that a model is robust. Robust to what? How is
linear regression non-robust?</p>
<p>In this post, we say a method is robust if its inferences aren’t (seriously)
affected by the presence of outliers.</p>
</div>
<div id="how-do-outliers-affect-linear-regression" class="section level2">
<h2>How do outliers affect linear regression?</h2>
<p>I think it will be easier to understand how outliers affect linear regressions
via an example based on the least squares method. This is not exactly how linear
regression works in our Bayesian world, but outlier’s bad consequences are similar.</p>
<p>In classic statistics, linear regression models are usually fitted by ordinary
least-squares method. This is equivalent to assuming the conditional distribution
of the response given the predictors is normal
(i.e. <span class="math inline">\(y_i|\boldsymbol{X}_i \sim N(\mu_i, \sigma)\)</span>) and using the maximum
likelihood estimator.</p>
<p>Let’s get started by simulating some toy data.</p>
<pre class="python"><code>np.random.seed(1234)

x = np.array([1., 2., 4., 5.])
y = x + np.random.normal(scale=0.5, size=4)</code></pre>
<p>Then, fit a linear regression between and visualize the result.</p>
<p>The next plot shows the data, the fitted line, and the contribution
of each data point to the total (squared) error as a blue square (one way to see
the least squares method is as the method that minimizes the sum of the areas of
the squares associated to all the points).</p>
<p><img src="/post/2021-07-05-robust-linear-regression-with-Bambi_files/figure-html/unnamed-chunk-3-1.png" width="95%" style="display: block; margin: auto;" /></p>
<p>So far so good! It looks like the fitted line is a good representation of the
relationship between the variables.</p>
<p>What happens if we introduce an outlier? In other words, what happens if there’s
a new point that deviates too much from the pattern we’ve just seen above? Let’s
see it!</p>
<pre class="python"><code>x = np.insert(x, 2, 2.25)
y = np.insert(y, 2, 5.8)</code></pre>
<p><img src="/post/2021-07-05-robust-linear-regression-with-Bambi_files/figure-html/unnamed-chunk-5-1.png" width="95%" style="display: block; margin: auto;" /></p>
<p>What a bummer! Why do we have such a huge error? It’s almost 10 times the
previous error with only one extra data point! Why?!</p>
<p>It happens that each point’s contribution to the error grows quadratically as it
moves away from the rest. Outliers not only contribute <strong>a lot</strong> to the total error,
they also bias the estimation towards themselves, increasing the error associated
with other points too. The final result? the fitted line is not a faithful
representation of the relationship between the variables.</p>
</div>
<div id="linear-regression-in-a-bayesian-way" class="section level2">
<h2>Linear regression in a Bayesian way</h2>
<p>Now that we’ve seen how bad outliers can be above, let’s see how one can
robust a Bayesian linear regression. This part of the post is based on the
<a href="https://docs.pymc.io/pymc-examples/examples/generalized_linear_models/GLM-robust.html">Robust Linear Regression</a>
in PyMC3 docs.</p>
<p>Here, we simulate data suitable for a normal linear regression and contaminate
it with a few outliers.</p>
<pre class="python"><code>size = 100
true_intercept = 1
true_slope = 2

x = np.linspace(0, 1, size)
true_regression_line = true_intercept + true_slope * x
y = true_regression_line + np.random.normal(scale=0.5, size=size)

x_out = np.append(x, [0.1, 0.15, 0.2])
y_out = np.append(y, [8, 6, 9])

data = pd.DataFrame(dict(x = x_out, y = y_out))</code></pre>
<div id="normal-linear-regression" class="section level3">
<h3>Normal linear regression</h3>
<p>The normal linear regression is as follows</p>
<p><span class="math display">\[
y_i \sim \text{Normal}(\mu_i, \sigma)
\]</span></p>
<p>where <span class="math inline">\(\mu_i = \beta_0 + \beta_1 x_i\)</span>, and the priors are of the form</p>
<p><span class="math display">\[
\begin{array}{c}
\beta_0 \sim \text{Normal} \\
\beta_1 \sim \text{Normal}  \\
\sigma \sim \text{HalfStudentT}
\end{array}
\]</span></p>
<p>with their parameters automatically set by Bambi.</p>
<pre class="python"><code>model = Model(&quot;y ~ x&quot;, data=data)
idata = model.fit()</code></pre>
<pre><code>## █
## Auto-assigning NUTS sampler...
## Initializing NUTS using jitter+adapt_diag...
## Multiprocess sampling (2 chains in 2 jobs)
## NUTS: [y_sigma, x, Intercept]
## Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds.</code></pre>
<p>To evaluate the fit, we use the posterior predictive regression lines. The line
in black is the true regression line.</p>
<p><img src="/post/2021-07-05-robust-linear-regression-with-Bambi_files/figure-html/plot1-1.png" width="95%" style="display: block; margin: auto;" /></p>
<p>As you can see, the posterior distribution fo the regression lines is not
centered around the true regression line, which means the estimations are
<strong>highly biased</strong>. This is the same phenomena we saw above with the least-squares
toy example.</p>
<p>Why does it happen here? The reason is that the normal distribution does
not have a lot of mass in the tails and consequently, an outlier will affect the
fit strongly.</p>
<p>Since the problem is the light tails of the Normal distribution we can instead
assume that our data is not normally distributed but instead distributed
according to the Student T distribution which has heavier tails as shown next.</p>
</div>
<div id="normal-and-student-t-distributions" class="section level3">
<h3>Normal and Student-T distributions</h3>
<p>Here we plot the pdf of a standard normal distribution and the pdf of a
student-t distribution with 3 degrees of freedom.</p>
<pre class="python"><code>x = np.linspace(-8, 8, num=200)
y_normal = stats.norm.pdf(x)
y_t = stats.t.pdf(x, df = 3)</code></pre>
<p><img src="/post/2021-07-05-robust-linear-regression-with-Bambi_files/figure-html/unnamed-chunk-8-1.png" width="95%" style="display: block; margin: auto;" /></p>
<p>As you can see, the probability of values far away from the mean are much more
likely under the Student-T distribution than under the Normal distribution.</p>
</div>
<div id="robust-linear-regression" class="section level3">
<h3>Robust linear regression</h3>
<p>The difference with the model above is that this one uses a StudentT likelihood
instead of a Normal one.</p>
<p>Bambi does not support yet to use the student-t distribution as the likelihood
function for linear regression. However, we can construct our own custom family
and Bambi will understand how to work with it.</p>
<p>Custom families are represented by the <a href="https://bambinos.github.io/bambi/master/api_reference.html#bambi.priors.Family"><code>Family</code></a>
class in Bambi. Let’s see what we need to create a custom family.</p>
<p>First of all, we need a name. In this case the name is going to be just <code>"t"</code>.
Second, there is the <code>prior</code> argument. This is where we pass the
model likelihood prior. Then we have the link function, which is simply the
identity function, and finally the parent indicates the name of the parameter for
the mean in the likelihood function, which in this case is <code>"mu"</code>.</p>
<pre class="python"><code>family = Family(
  name = &quot;t&quot;, 
  prior = Prior(&quot;StudentT&quot;, sigma=1, nu=2), 
  link = &quot;identity&quot;, 
  parent = &quot;mu&quot;
)

# In addition, we pass our custom priors for the terms in the model.
priors = {
  &quot;Intercept&quot;: Prior(&quot;Normal&quot;, mu=0, sigma=10),
  &quot;x&quot;: Prior(&quot;Normal&quot;, mu=0, sigma=10)
}

# Just add the `prior` and `family` arguments
model = Model(&quot;y ~ x&quot;, data, priors=priors, family=family)
idata = model.fit()</code></pre>
<pre><code>## █
## Auto-assigning NUTS sampler...
## Initializing NUTS using jitter+adapt_diag...
## Multiprocess sampling (2 chains in 2 jobs)
## NUTS: [x, Intercept]
## Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds.
## The number of effective samples is smaller than 25% for some parameters.</code></pre>
<p><img src="/post/2021-07-05-robust-linear-regression-with-Bambi_files/figure-html/plot2-1.png" width="95%" style="display: block; margin: auto;" /></p>
<p>Much better now! The posterior distribution of the regression lines is almost
centered around the true regression line, and uncertainty has decreased,
that’s great! The outliers are barely influencing our estimation because our
likelihood function assumes that outliers are much more probable than under the
Normal distribution.</p>
</div>
</div>
