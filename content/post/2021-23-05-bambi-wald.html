---
title: "Why Bambi?"
author: "Tomás Capretto"
date: 2021-05-23
output: html_document
description: A quick example comparing how to fit a GLM with Bambi and PyMC3
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>I’ve been thinking about writing a new blog post for a while now but honestly
there was nothing coming to my mind that made me think “Oh, yeah, this is
interesting, it can be useful for someone else”.
And it was just a few hours ago that I realized I could write about something
quite curious that happened to me while trying to replicate a
<a href="https://bambinos.github.io/bambi/">Bambi</a> model with <a href="https://docs.pymc.io/">PyMC3</a>.</p>
<p>A couple of weeks ago <a href="https://twitter.com/AgustinaArroyu1">Agustina Arroyuelo</a>
told me she was trying to replicate a <a href="https://bambinos.github.io/bambi/master/notebooks/wald_gamma_glm.html#Wald">model</a>
in an example notebook in Bambi and wanted my opinion on what she was doing.
After many attempts, neither of us could replicate the model successfully. It
turned out to be we were messing up with the shapes of the priors and also had
some troubles with the design matrix.</p>
<p>The point of this post is not about good practices when doing Bayesian modeling
neither about modeling techniques. This simply aims to show how Bambi can save
us a lot of time, effort, and code when fitting not-so-trivial GLMs in Python.</p>
<p>Well, I think this is quite enough for an introduction. Let’s better have a look
at the problem at hand.</p>
</div>
<div id="setup" class="section level2">
<h2>Setup</h2>
<p>We use the <code>reticulate</code> package to enable Python in our R Markdown document.
<code>"rbambi"</code> is just a conda environment that has the latest version of Bambi
installed.</p>
<pre class="r"><code>library(ggplot2)
library(reticulate)
use_condaenv(&quot;rbambi&quot;, required = TRUE)</code></pre>
<pre class="python"><code>import arviz as az
import bambi as bmb
import numpy as np
import pandas as pd
import pymc3 as pm
import theano.tensor as tt</code></pre>
</div>
<div id="the-problem" class="section level2">
<h2>The problem</h2>
<p>In this problem we use a data set consisting of 67856 insurance policies and
4624 (6.8%) claims in Australia between 2004 and 2005.
The original source of this dataset is the book <a href="http://www.businessandeconomics.mq.edu.au/our_departments/Applied_Finance_and_Actuarial_Studies/research/books/GLMsforInsuranceData">Generalized Linear Models for
Insurance Data</a>
by Piet de Jong and Gillian Z. Heller.</p>
<pre class="python"><code>data = pd.read_csv(
  &quot;https://courses.ms.ut.ee/2020/glm/spring/uploads/Main/carclaims.csv&quot;
)
data = data[data[&quot;claimcst0&quot;] &gt; 0]</code></pre>
<p>The age (binned), the gender, and the area of residence are used to predict the
amount of the claim, conditional on the existence of the claim, because we are
only working with observations where there is a claim.</p>
<p>We use a Wald regression model. This is a GLM where the random component follows
a <a href="https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution">Wald distribution</a>.
The link function we choose is the natural logarithm.</p>
</div>
<div id="pymc3-model" class="section level2">
<h2>PyMC3 model</h2>
<div id="data-preparation" class="section level3">
<h3>Data preparation</h3>
<p>To fit the model with PyMC3 we first need to create the model matrix. We need
to represent <code>age</code>, <code>area</code>, and <code>gender</code> with dummy variables because they are
categoric. We can think of the following objects as sub-matrices of the
design matrix in the model.</p>
<pre class="python"><code>intercept = np.ones((len(data), 1))
age = pd.get_dummies(data[&quot;agecat&quot;], drop_first=True).to_numpy()
area = pd.get_dummies(data[&quot;area&quot;], drop_first=True).to_numpy()
gender = pd.get_dummies(data[&quot;gender&quot;], drop_first=True).to_numpy()</code></pre>
<p>Then we just stack horizontally these sub-matrices and convert the result to a
Theano tensor variable so we can compute the dot product between this matrix and
the vector of coefficients when writing our model in PyMC3.</p>
<pre class="python"><code>X = np.hstack([intercept, age, gender, area])
X = tt.as_tensor_variable(X)</code></pre>
</div>
<div id="fit" class="section level3">
<h3>Fit</h3>
<p>We start declaring the priors for each of the predictors in the model. They are
all independent Gaussian distributions. You may wonder where I took the values
for the parameters of these distributions. I’ve just copied Bambi’s default
values for this particular problem.</p>
<p>At this stage, it is <strong>very important</strong> to give appropiate shapes to all the
objects we create in the model. For example, <code>β_age</code> is a random variable that
represents the coefficients for the age variable. Since 5 dummy variables are
used to represent the age, <code>β_age</code> has <code>shape=(5, 1)</code>, as well as the arrays
passed to <code>mu</code> and <code>sigma</code>.</p>
<pre class="python"><code># Create model and sample posterior
with pm.Model() as model_pymc3:  
    # Build predictors
    β_0 = pm.Normal(
        &quot;β_0&quot;,
        mu=np.array([[7.61]]),
        sigma=np.array([[2.73]]),
        shape=(1, 1)
    )
    β_age = pm.Normal(
        &quot;β_age&quot;,
        mu=np.array([[0] * 5]).T,
        sigma=np.array([[0.32, 6.94, 1.13, 5.44, 9.01]]).T,
        shape=(5, 1)
    )
    β_gender = pm.Normal(
        &quot;β_gender&quot;,
        mu=np.array([[0]]),
        sigma=np.array([[1.304491]]),
        shape=(1, 1)
    )
    β_area = pm.Normal(
      &quot;β_area&quot;,
      mu=np.array([[0] * 5]).T,
      sigma=np.array([[0.86, 0.25, 1.3, 0.76, 5.33]]),
      shape=(5, 1)
    )
    
    # Compute linear predictor
    β = tt.concatenate([β_0, β_age, β_gender, β_area], axis=0)
    
    # Transform linear predictor
    mu = tt.exp(X.dot(β))
      
    response = np.array([data[&quot;claimcst0&quot;]]).T
    pm.Wald(
      &quot;claim&quot;, 
      mu=mu, 
      lam=pm.HalfCauchy(&quot;claim_lam&quot;, beta=1), 
      observed=response
    )
    idata_pymc = pm.sample(
      tune=2000, 
      draws=4000, 
      random_seed=1234,
      return_inferencedata=True
    )</code></pre>
<pre><code>## claim
## █
## Auto-assigning NUTS sampler...
## Initializing NUTS using jitter+adapt_diag...
## Multiprocess sampling (2 chains in 2 jobs)
## NUTS: [claim_lam, β_area, β_gender, β_age, β_0]
## Sampling 2 chains for 2_000 tune and 4_000 draw iterations (4_000 + 8_000 draws total) took 38 seconds.
## The number of effective samples is smaller than 25% for some parameters.</code></pre>
</div>
</div>
<div id="bambi-model" class="section level2">
<h2>Bambi model</h2>
<p>As you can see below, we don’t need to prepare any data, or write priors by
hand. Bambi automatically obtains sensible default priors when they are not
specified, and knows how to handle each variable type very well.</p>
<p>The model is specified using a model formula, quite similar to model formulas
in R. The left-hand side of <code>~</code> is the response variable, and the rest
are the predictors. Here <code>C(agecat)</code> tells Bambi that <code>agecat</code> should be
interpreted as categoric. The <code>family</code> and <code>link</code> arguments are quite
self-explanatory.</p>
<p>Then we have the <code>.fit()</code> method, where you can pass arguments to the
<code>pm.sample()</code> function that’s running on the background.</p>
<pre class="python"><code>model_bambi= bmb.Model(
  &quot;claimcst0 ~ C(agecat) + gender + area&quot;, 
  data, 
  family = &quot;wald&quot;, 
  link = &quot;log&quot;
)
idata_bambi = model_bambi.fit(tune=2000, draws=4000, random_seed=1234)</code></pre>
<pre><code>## █
## Auto-assigning NUTS sampler...
## Initializing NUTS using jitter+adapt_diag...
## Multiprocess sampling (2 chains in 2 jobs)
## NUTS: [claimcst0_lam, area, gender, C(agecat), Intercept]
## Sampling 2 chains for 2_000 tune and 4_000 draw iterations (4_000 + 8_000 draws total) took 36 seconds.</code></pre>
<p>And that’s it! A model that took several lines of codes to specify in PyMC3
only took a few lines of code in Bambi. Quite an advantage, right?</p>
</div>
<div id="check-results" class="section level2">
<h2>Check results</h2>
<p>The simplicity we gain with Bambi would be worthless if the results turned out
to be different. We want an interface that makes our job easier, without
affecting the quality of the inference. The following is a forest plot
where the point gives the posterior mean and the bars indicate a 94% HDI.</p>
<p><img src="/post/2021-23-05-bambi-wald_files/figure-html/unnamed-chunk-10-1.png" width="100%" style="display: block; margin: auto;" /></p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>In this post, we saw how a model in PyMC3 can be re-written in Bambi using a
much simpler syntax. Bambi saved us time and code and still produced the same
result as in PyMC3.</p>
<p>Nevertheless, this doesn’t mean we should always favor Bambi over PyMC3.
Not at all! Bambi has still to mature a lot and is not even close to being as
flexible as PyMC3 is. The take-home message is that while PyMC3 is awesome and
will be the way to go in most cases, Bambi’s concise syntax is a tremendous
advantage when fitting not-so-trivial GLMs like the one shown here.</p>
</div>
