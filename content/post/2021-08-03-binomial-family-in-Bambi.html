---
title: "Binomial family in Bambi"
author: "Tomás Capretto"
date: 2021-08-03
output: html_document
description: My fourth post describing work done during GSoC 2021. On this 
  occasion, I'm introducing the Binomial family. This new family is very useful 
  to build models for binary data when each row in the data set contains the 
  number of successes and the number of trials instead of the results of 
  Bernoulli trials.
editor_options:
  chunk_output_type: console
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<hr />
<p>Although GSoC 2021 is close to come to an end, there’s still a lot of exciting
things going on around <a href="https://bambinos.github.io/bambi">Bambi</a>. Today I’m
going to talk about another new family that’s about to be merged into the main
branch, the Binomial family.</p>
<p>Let’s get started by trying to see why we need to have another new family for
modeling binary data in Bambi.</p>
</div>
<div id="aggregated-vs-disaggregated-data" class="section level2">
<h2>Aggregated vs disaggregated data</h2>
<hr />
<p>Bambi already has the Bernoulli family to model binary data.
This family fits very well when you have a data set where each row represents a
single observation and there’s a column that represents the binary outcome (
i.e the result of the Bernoulli trial) as well as other columns with the
predictor variables.</p>
<p>Let’s say we want to study the lethality of a certain drug and we have a group
of mice to experiment with. An approach could be to divide the mice into smaller
groups, assign a certain dose to all the mice in each group, and then finally
count the number of units that died after a fixed amount of time. Under
the Bernoulli family paradigm, each row has to represent a single observation,
looking like this:</p>
<table>
<thead>
<tr class="header">
<th align="right">Obs</th>
<th align="right">Dose</th>
<th align="right">Died</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">1.3</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">1.8</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">2.2</td>
<td align="right">1</td>
</tr>
</tbody>
</table>
<p><br></p>
<p>where each row represents a single mouse (i.e. a single Bernoulli trial).
The <strong>0</strong> is used to represent a <strong>failure/survival</strong>, and <strong>1</strong> is used to
represent a <strong>successes/death</strong>.</p>
<p>What if our data is aggregated? The nature of the experiment makes it natural
to have rows representing groups, a column representing the number of deaths,
and another column representing the number of mice in the group.</p>
<table>
<thead>
<tr class="header">
<th align="right">Group</th>
<th align="right">Dose</th>
<th align="right">Dead</th>
<th align="right">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">1.3</td>
<td align="right">12</td>
<td align="right">20</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">1.8</td>
<td align="right">18</td>
<td align="right">25</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">2.2</td>
<td align="right">24</td>
<td align="right">34</td>
</tr>
</tbody>
</table>
<p><br></p>
<p>where each row represents a group of mice. Dose is the dose applied to all the
units in the group, Dead is the number of mice that died, and
Total is the number of mice in the group. If we focus on the <strong>Dead</strong> and
<strong>Total</strong> columns we can easily see they resemble data coming from
a <strong>Binomial distribution</strong> (i.e. number of successes out of a series of <span class="math inline">\(n\)</span>
independent Bernoulli trials). In other words, for a given row, we can think
there’s a Binomial distribution where <strong>Dead</strong> represents the number of
successes out of <strong>Total</strong> number of trials (each mouse is a trial).</p>
<p>Before continuing, it’s important to note that if the data is originally
aggregated as in the lower table, it can always be disaggregated to resemble
the one in the upper table. So what’s the problem?</p>
<p>The answer is that there’s actually nothing wrong with having the data in such
a granular form! But, if the data already comes aggregated, why doing extra
work when we now have the Binomial family? Let’s have a look at the examples
below!</p>
</div>
<div id="setup" class="section level2">
<h2>Setup</h2>
<hr />
<pre class="python"><code>import arviz as az
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

from bambi import Model, Prior

az.style.use(&quot;arviz-darkgrid&quot;)

BLUE = &quot;#003f5c&quot;
PURPLE = &quot;#7a5195&quot;
PINK = &quot;#ef5675&quot;</code></pre>
<p>We’re going to use real data in this example<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>.
This data consists of the numbers of beetles dead after five hours of
exposure to gaseous carbon disulphide at various concentrations:</p>
<table>
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th>Dose, <span class="math inline">\(x_i\)</span> <br />(<span class="math inline">\(\log_{10}\text{CS}_2\text{mgl}^{-1}\)</span>)</th>
<th>Number of beetles, <span class="math inline">\(n_i\)</span></th>
<th>Number killed, <span class="math inline">\(y_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1.6907</td>
<td>59</td>
<td>6</td>
</tr>
<tr class="even">
<td>1.7242</td>
<td>60</td>
<td>13</td>
</tr>
<tr class="odd">
<td>1.7552</td>
<td>62</td>
<td>18</td>
</tr>
<tr class="even">
<td>1.7842</td>
<td>56</td>
<td>28</td>
</tr>
<tr class="odd">
<td>1.8113</td>
<td>63</td>
<td>52</td>
</tr>
<tr class="even">
<td>1.8369</td>
<td>59</td>
<td>53</td>
</tr>
<tr class="odd">
<td>1.8610</td>
<td>62</td>
<td>61</td>
</tr>
<tr class="even">
<td>1.8839</td>
<td>60</td>
<td>60</td>
</tr>
</tbody>
</table>
<p><br>
And now let’s write it down into a data frame:</p>
<pre class="python"><code>x = np.array([1.6907, 1.7242, 1.7552, 1.7842, 1.8113, 1.8369, 1.8610, 1.8839])
n = np.array([59, 60, 62, 56, 63, 59, 62, 60])
y = np.array([6, 13, 18, 28, 52, 53, 61, 60])

data = pd.DataFrame({
    &quot;x&quot;: x,
    &quot;y&quot;: y,
    &quot;n&quot;: n
})</code></pre>
<p>Quite simple, right? Can we use it as it is with the Bernoulli family? Let’s have a look below.</p>
</div>
<div id="bernoulli-family" class="section level2">
<h2>Bernoulli family</h2>
<hr />
<p>Nope, no surprises today. To use the Bernoulli family, we first need to
transform the data into the dissagregated or long format. One approach is the
following</p>
<pre class="python"><code>data_bernoulli = pd.DataFrame({
    &quot;x&quot;: np.concatenate([np.repeat(x, n) for x, n in zip(x, n)]),
    &quot;killed&quot;: np.concatenate([np.repeat([1, 0], [y, n - y]) for y, n in zip(y, n)])
})</code></pre>
<p>Do you realize how bothering it can be to do that if we have many more
variables? Nevermind, let’s keep going.</p>
<p>Now let’s initialize a Bambi model and sample from the posterior:</p>
<pre class="python"><code>model_brn = Model(&quot;killed ~ x&quot;, data_bernoulli, family=&quot;bernoulli&quot;)
idata_brn = model_brn.fit()</code></pre>
<pre><code>## █
## Modeling the probability that killed==1
## Auto-assigning NUTS sampler...
## Initializing NUTS using jitter+adapt_diag...
## Multiprocess sampling (2 chains in 2 jobs)
## NUTS: [Intercept, x]
## Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds.</code></pre>
<p>and explore the marginal posteriors</p>
<pre class="python"><code>az.summary(idata_brn, kind=&quot;stats&quot;)</code></pre>
<pre><code>##              mean     sd  hdi_3%  hdi_97%
## Intercept -61.141  5.015 -70.230  -51.811
## x          34.510  2.818  29.438   39.787</code></pre>
<p>We can predict the the probability of dying for out-of-sample data to see how
it evolves with the different concentration levels.</p>
<pre class="python"><code>new_data = pd.DataFrame({&quot;x&quot;: np.linspace(1.6, 2, num=200)})
model_brn.predict(idata_brn, data=new_data)</code></pre>
<p>Let’s visualize the mean of the probability of dying as well as the HDI for
the posterior of this probability:</p>
<pre class="python"><code>fig, ax = plt.subplots()

# Plot HDI for the mean of the probability of dying
az.plot_hdi(
  new_data[&quot;x&quot;], 
  idata_brn.posterior[&quot;killed_mean&quot;].values, 
  color=BLUE,
  ax=ax
);

ax.plot(
  new_data[&quot;x&quot;], 
  idata_brn.posterior[&quot;killed_mean&quot;].values.mean((0, 1)), 
  color=BLUE
);

ax.scatter(x, y / n, s=50, color=PURPLE, edgecolors=&quot;black&quot;, zorder=10);
ax.set_ylabel(&quot;Probability of death&quot;);
ax.set_xlabel(r&quot;Dose $\log_{10}CS_2mgl^{-1}$&quot;);
ax.set_title(&quot;family=&#39;bernoulli&#39;&quot;);
plt.show()</code></pre>
<p><img src="/post/2021-08-03-binomial-family-in-Bambi_files/figure-html/unnamed-chunk-1-1.png" width="95%" style="display: block; margin: auto;" /></p>
</div>
<div id="binomial-family" class="section level2">
<h2>Binomial family</h2>
<hr />
<p>Before writing down the model with the Binomial family, let’s take a moment to
review new notation that was added specifically for this purpose.</p>
<p>The model formula syntax only allows us to pass one variable on its LHS. Then,
how do we tell Bambi that what we want to model is the <strong>proportion</strong> that
results from dividing <strong>y</strong> over <strong>n</strong>?</p>
<p>Thanks to recent developments, it’s as easy as writing <code>proportion(y, n)</code>, or
any of its aliases <code>prop(y, n)</code> and <code>p(y, n)</code>. To keep it shorter, let’s use the
last one.</p>
<pre class="python"><code>model_bnml = Model(&quot;p(y, n) ~ x&quot;, data, family=&quot;binomial&quot;)
idata_bnml = model_bnml.fit()</code></pre>
<pre><code>## █
## Auto-assigning NUTS sampler...
## Initializing NUTS using jitter+adapt_diag...
## Multiprocess sampling (2 chains in 2 jobs)
## NUTS: [Intercept, x]
## Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 2 seconds.</code></pre>
<p>Quite simple, right? The code here is very similar to the one for the model with
the Bernoulli family. However, the new Binomial family allows us to
<strong>use the data in its original form</strong>.</p>
<p>Let’s finish this section by getting the marginal posteriors as well as a figure
as the one displayed above.</p>
<pre class="python"><code>az.summary(idata_bnml, kind=&quot;stats&quot;)</code></pre>
<pre><code>##              mean     sd  hdi_3%  hdi_97%
## Intercept -61.023  5.130 -70.040  -51.145
## x          34.442  2.884  28.914   39.571</code></pre>
<pre class="python"><code>model_bnml.predict(idata_bnml, data=new_data)</code></pre>
<pre class="python"><code>fig, ax = plt.subplots()

az.plot_hdi(
  new_data[&quot;x&quot;],
  idata_bnml.posterior[&quot;p(y, n)_mean&quot;].values,
  color=BLUE,
  ax=ax
);

ax.plot(
  new_data[&quot;x&quot;], 
  idata_bnml.posterior[&quot;p(y, n)_mean&quot;].values.mean((0, 1)), 
  color=BLUE
);

ax.scatter(x, y / n, s=50, color=PURPLE, edgecolors=&quot;black&quot;, zorder=10);
ax.set_ylabel(&quot;Probability of death&quot;);
ax.set_xlabel(r&quot;Dose $\log_{10}CS_2mgl^{-1}$&quot;);
ax.set_title(&quot;family=&#39;binomial&#39;&quot;);
plt.show()</code></pre>
<p><img src="/post/2021-08-03-binomial-family-in-Bambi_files/figure-html/unnamed-chunk-1-3.png" width="95%" style="display: block; margin: auto;" /></p>
</div>
<div id="conclusions" class="section level2">
<h2>Conclusions</h2>
<hr />
<p>This blog post introduced the new Binomial family. This new family saves us
from having to manipulate aggregated data prior to modeling, making it more
pleasant and simpler to specify and fit models for binary data in Bambi.</p>
<!-- ## Other additions -->
<!-- *** -->
<!-- One cool addition is that Bambi nows internally centers the predictors when -->
<!-- the model contains an intercept. This usually improves the efficiency of the -->
<!-- sampling, so expect a speedup in the next release! -->
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>This data can be found in An Introduction to Generalized Linear Models by A. J. Dobson and A. G. Barnett, but the original source is (Bliss, 1935).<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
