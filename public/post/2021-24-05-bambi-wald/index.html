<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="generator" content="Hugo 0.83.1" />

  <title>Why Bambi? &middot; Tomás Capretto</title>

  <meta name="description" content="An example comparing how to fit a GLM with Bambi and PyMC3. Here I attempt to highlight how Bambi can help us to write a Bayesian GLM in a concise manner, saving us from having to realize error-prone tasks that are sometimes necessary when directly working with PyMC3." />

  
  <meta property="og:locale" content="en"/>

  
  <meta property="og:image" content="/images/profile_pic.png">

  
  <meta property="og:site_name" content="Tomás Capretto"/>
  <meta property="og:title" content="Why Bambi?"/>
  <meta property="og:description" content="An example comparing how to fit a GLM with Bambi and PyMC3. Here I attempt to highlight how Bambi can help us to write a Bayesian GLM in a concise manner, saving us from having to realize error-prone tasks that are sometimes necessary when directly working with PyMC3."/>
  <meta property="og:url" content="/post/2021-24-05-bambi-wald/"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2021-05-24T00:00:00Z"/>
  <meta property="article:modified_time" content="2021-05-24T00:00:00Z"/>
  <meta property="article:author" content="">
  
  
  

  <script type="application/ld+json">
  {
    "@context" : "http://schema.org",
    "@type" : "Blog",
    "name": "Tomás Capretto",
    "url" : "/",
    "image": "/images/profile_pic.png",
    "description": "PhD candidate, Statistics"
  }
  </script>
  
  <link rel="stylesheet" href=/css/lioshi.css id="theme-stylesheet">
  <script src=/js/highlight.pack.js></script>
  <script>hljs.initHighlightingOnLoad();</script>

  
  <script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "name": "Why Bambi?",
    "headline": "Why Bambi?",
    "datePublished": "2021-05-24T00:00:00Z",
    "dateModified": "2021-05-24T00:00:00Z",
    "author": {
      "@type": "Person",
      "name": "",
      "url": "/"
    },
    "image": "/images/profile_pic.png",
    "url": "/post/2021-24-05-bambi-wald/",
    "description": "An example comparing how to fit a GLM with Bambi and PyMC3. Here I attempt to highlight how Bambi can help us to write a Bayesian GLM in a concise manner, saving us from having to realize error-prone tasks that are sometimes necessary when directly working with PyMC3."
  }
  </script>
  


  <link type="text/css"
        rel="stylesheet"
        href="/css/print.css"
        media="print">

  <link type="text/css"
        rel="stylesheet"
        href="/css/poole.css">

  <link type="text/css"
        rel="stylesheet"
        href="/css/hyde.css">
				
  <link type="text/css"
        rel="stylesheet"
        href="/css/tcapretto.css">				

  


  

  <link rel="stylesheet"
        href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700&display=swap">

  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css"
        integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk="
        crossorigin="anonymous" />

  <link rel="apple-touch-icon-precomposed"
        sizes="144x144"
        href="/apple-touch-icon-144-precomposed.png">

  <link rel="shortcut icon" href="/favicon.png">

  
  </head>
<body>
  <aside class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      
        
        <div class="author-image">
          <img src="/images/profile_pic.png" class="img-circle img-headshot center" alt="Profile Picture">
        </div>
        
      

      <h1>Tomás Capretto</h1>
      
      <h3>PhD candidate, Statistics </h3>
      <h3>IMASL (CONICET - UNSL) </h3>
      
      
      
      
      
      
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li>
          <a href="/">Home</a>
        </li>
        <li>
          <a href="/post/"> Posts </a>
        </li><li>
          <a href="/slides/"> Slides </a>
        </li><li>
          <a href="/about/"> About </a>
        </li>
      </ul>
    </nav>

    <section class="social-icons">
      
      <a href="https://twitter.com/CaprettoTomas" rel="me" title="Twitter">
        <i class="fab fa-twitter" aria-hidden="true"></i>
      </a>
      
      <a href="https://github.com/tomicapretto" rel="me" title="GitHub">
        <i class="fab fa-github" aria-hidden="true"></i>
      </a>
      
      <a href="https://stackoverflow.com/users/12266277/tomas-capretto" rel="me" title="Stack Overflow">
        <i class="fab fa-stack-overflow" aria-hidden="true"></i>
      </a>
      
      <a href="https://github.com/tomicapretto/cv/raw/master/cv-english/cv-english.pdf" rel="me" title="Curriculum Vitae">
        <i class="fab far fa-file-alt" aria-hidden="true"></i>
      </a>
      
    </section>
  </div>
</aside>


  <main class="content container">
  <div class="post">
  <h1>Why Bambi?</h1>

  <div class="post-date">
    <time datetime="2021-05-24T00:00:00Z">May 24, 2021</time> · 8 min read
  </div>

  
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>I’ve been thinking about writing a new blog post for a while now but honestly,
there was nothing coming to my mind that made me think “Oh, yeah, this is
interesting, it can be useful for someone else”.
And it was just a few hours ago that I realized I could write about something
quite curious that happened to me while trying to replicate a
Bambi model with PyMC3.</p>
<p><a href="https://docs.pymc.io/">PyMC3</a> is a Python package for Bayesian statistical
modeling that implements advanced Markov chain Monte Carlo algorithms, such as
the No-U-Turn sampler (NUTS). <a href="https://bambinos.github.io/bambi/">Bambi</a> is a
high-level Bayesian model-building interface in Python. It is built on top of
PyMC3 and allows users to specify and fit Generalized Linear Models (GLMs) and
Generalized Linear Mixed Models (GLMMs) very easily using a model formula much
similar to the popular model formulas in R.</p>
<p>A couple of weeks ago <a href="https://twitter.com/AgustinaArroyu1">Agustina Arroyuelo</a>
told me she was trying to replicate a <a href="https://bambinos.github.io/bambi/master/notebooks/wald_gamma_glm.html#Wald">model</a>
in one of the example notebooks we have in Bambi and wanted my opinion on what
she was doing. After many attempts, neither of us could replicate the model
successfully. It turned out to be we were messing up with the shapes of the
priors and also had some troubles with the design matrix.</p>
<p>The point of this post is not about good practices when doing Bayesian modeling
neither about modeling techniques. This post aims to show how Bambi can save you
effort, code, and prevent us from making some mistakes when fitting
not-so-trivial GLMs in Python.</p>
<p>Well, I think this is quite enough for an introduction. Let’s better have a look
at the problem at hand.</p>
</div>
<div id="setup" class="section level2">
<h2>Setup</h2>
<p>We use the <code>reticulate</code> R package to enable Python in our R Markdown document.
<code>"rbambi"</code> is a conda environment that has the latest version of Bambi
installed <a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>.</p>
<pre class="r"><code>library(ggplot2)
library(reticulate)
use_condaenv(&quot;rbambi&quot;, required = TRUE)</code></pre>
<pre class="python"><code>import arviz as az
import bambi as bmb
import numpy as np
import pandas as pd
import pymc3 as pm
import theano.tensor as tt</code></pre>
</div>
<div id="the-problem" class="section level2">
<h2>The problem</h2>
<p>In this problem we use a data set consisting of 67856 insurance policies and
4624 (6.8%) claims in Australia between 2004 and 2005.
The original source of this dataset is the book <a href="http://www.businessandeconomics.mq.edu.au/our_departments/Applied_Finance_and_Actuarial_Studies/research/books/GLMsforInsuranceData">Generalized Linear Models for
Insurance Data</a> by Piet de Jong and Gillian Z. Heller.</p>
<pre class="python"><code>data = pd.read_csv(
  &quot;https://courses.ms.ut.ee/2020/glm/spring/uploads/Main/carclaims.csv&quot;
)
data = data[data[&quot;claimcst0&quot;] &gt; 0]</code></pre>
<p>The age (binned), the gender, and the area of residence are used to predict the
amount of the claim, conditional on the existence of the claim because we are
only working with observations where there is a claim.</p>
<p>We use a Wald regression model. This is a GLM where the random component follows
a <a href="https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution">Wald distribution</a>.
The link function we choose is the natural logarithm.</p>
</div>
<div id="pymc3-model" class="section level2">
<h2>PyMC3 model</h2>
<div id="data-preparation" class="section level3">
<h3>Data preparation</h3>
<p>To fit the model with PyMC3 we first need to create the model matrix. We need
to represent <code>age</code>, <code>area</code>, and <code>gender</code> with dummy variables because they are
categorical. We can think of the following objects as sub-matrices of the
design matrix in the model.</p>
<pre class="python"><code>intercept = np.ones((len(data), 1))
age = pd.get_dummies(data[&quot;agecat&quot;], drop_first=True).to_numpy()
area = pd.get_dummies(data[&quot;area&quot;], drop_first=True).to_numpy()
gender = pd.get_dummies(data[&quot;gender&quot;], drop_first=True).to_numpy()</code></pre>
<p>Note we have used <code>drop_first=True</code>. This means that we use <code>n_levels - 1</code>
dummies to represent each categorical variable, and the first level is taken as
reference. This ensures the resulting design matrix is of full rank.</p>
<p>Next, we stack these sub-matrices horizontally and convert the result to a
Theano tensor variable so we can compute the dot product between this matrix and
the vector of coefficients when writing our model in PyMC3.</p>
<pre class="python"><code>X = np.hstack([intercept, age, gender, area])
X = tt.as_tensor_variable(X)</code></pre>
</div>
<div id="fit" class="section level3">
<h3>Fit</h3>
<p>We start declaring the priors for each of the predictors in the model. They are
all independent Gaussian distributions. You may wonder where I took the values
for the parameters of these distributions. I’ve just copied Bambi’s default
values for this particular problem.</p>
<p>At this stage, it is <strong>very important</strong> to give appropriate shapes to all the
objects we create in the model. For example, <code>β_age</code> is a random variable that
represents the coefficients for the age variable. Since 5 dummy variables are
used to represent the age, both <code>β_age</code> and the values passed to <code>mu</code> and
<code>sigma</code> must have <code>shape=(5, 1)</code>. <a href="https://discourse.pymc.io/t/create-model-matrix/7429">I’ve failed here many times</a>
when trying to replicate the model, so, unfortunately, I know what I’m talking
about 😅</p>
<pre class="python"><code># Create model and sample posterior
with pm.Model() as model_pymc3:  
    # Build predictors
    β_0 = pm.Normal(
        &quot;β_0&quot;,
        mu=np.array([[7.61]]),
        sigma=np.array([[2.73]]),
        shape=(1, 1)
    )
    β_age = pm.Normal(
        &quot;β_age&quot;,
        mu=np.array([[0] * 5]).T,
        sigma=np.array([[0.32, 6.94, 1.13, 5.44, 9.01]]).T,
        shape=(5, 1)
    )
    β_gender = pm.Normal(
        &quot;β_gender&quot;,
        mu=np.array([[0]]),
        sigma=np.array([[1.304491]]),
        shape=(1, 1)
    )
    β_area = pm.Normal(
      &quot;β_area&quot;,
      mu=np.array([[0] * 5]).T,
      sigma=np.array([[0.86, 0.25, 1.3, 0.76, 5.33]]),
      shape=(5, 1)
    )
    
    # Concatenate the vectors for the coefficients into a single vector
    β = tt.concatenate([β_0, β_age, β_gender, β_area], axis=0)
    
    # Compute and transform linear predictor
    mu = tt.exp(X.dot(β))
      
    response = np.array([data[&quot;claimcst0&quot;]]).T
    pm.Wald(
      &quot;claim&quot;, 
      mu=mu, 
      lam=pm.HalfCauchy(&quot;claim_lam&quot;, beta=1), 
      observed=response
    )
    idata_pymc = pm.sample(
      tune=2000, draws=4000, target_accept=0.9, random_seed=1234,
      return_inferencedata=True
    )</code></pre>
<pre><code>## claim
## █
## Auto-assigning NUTS sampler...
## Initializing NUTS using jitter+adapt_diag...
## Multiprocess sampling (2 chains in 2 jobs)
## NUTS: [claim_lam, β_area, β_gender, β_age, β_0]
## Sampling 2 chains for 2_000 tune and 4_000 draw iterations (4_000 + 8_000 draws total) took 35 seconds.</code></pre>
</div>
</div>
<div id="bambi-model" class="section level2">
<h2>Bambi model</h2>
<p>As you can see below, we don’t need to do any data preparation, or even
specify priors by hand. Bambi automatically obtains sensible default priors when
they are not specified, and also knows how to handle each variable type very
well.</p>
<p>The model is specified using a model formula, quite similar to model formulas
in R. The left-hand side of <code>~</code> is the response variable, and the rest
are the predictors. Here <code>C(agecat)</code> tells Bambi that <code>agecat</code> should be
interpreted as categorical. The <code>family</code> argument indicates the conditional
distribution for the response, and the <code>link</code> tells Bambi which function of the
mean is being modeled by the linear predictor. More information about how they
work can be found <a href="https://bambinos.github.io/bambi/master/notebooks/getting_started.html#Families">here</a>.</p>
<p>Then we have the <code>.fit()</code> method, where you can pass arguments to the
<code>pm.sample()</code> function that’s running in the background.</p>
<pre class="python"><code>model_bambi= bmb.Model(
  &quot;claimcst0 ~ C(agecat) + gender + area&quot;, 
  data, 
  family = &quot;wald&quot;, 
  link = &quot;log&quot;
)
idata_bambi = model_bambi.fit(tune=2000, draws=4000, target_accept=0.9, random_seed=1234)</code></pre>
<pre><code>## █
## Auto-assigning NUTS sampler...
## Initializing NUTS using jitter+adapt_diag...
## Multiprocess sampling (2 chains in 2 jobs)
## NUTS: [claimcst0_lam, area, gender, C(agecat), Intercept]
## Sampling 2 chains for 2_000 tune and 4_000 draw iterations (4_000 + 8_000 draws total) took 38 seconds.</code></pre>
<p>And that’s it! A model that took several lines of codes to specify in PyMC3
only took a few lines of code in Bambi. Quite an advantage, right?</p>
</div>
<div id="check-results" class="section level2">
<h2>Check results</h2>
<p>The simplicity we gain with Bambi would be worthless if the results turned out
to be different. We want an interface that makes our job easier, without
affecting the quality of the inference. The following is a forest plot
where the point gives the posterior mean and the bars indicate a 94% HDI.</p>
<p><img src="/post/2021-24-05-bambi-wald_files/figure-html/unnamed-chunk-8-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>While most of the marginal posteriors match very well, we can clearly see the
ones for <code>β_area[3]</code> and <code>β_area[4]</code> don’t overlap as much as the others.
One of the possible explanations for this difference is related to the MCMC
algorithm. While we know both models are indeed the same model, their internal
representation is not exactly the same. For example, the model we wrote in pure
PyMC3 computes a unique dot product between a matrix of shape <code>(n, p)</code> a vector
of shape <code>(p, 1)</code>, while the model in Bambi is computing the sum of many smaller
dot products. As the internal representations are not exactly the same, the
sampling spaces differ and the sampling algorithm obtained slightly different
results.</p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>In this post, we saw how the same GLM can be expressed in both PyMC3 and Bambi.
PyMC3 allowed us to control every fine-grained detail of the model specification,
while Bambi allowed us to express the same model in a much more concise manner.</p>
<p>Bambi’s advantages in these types of scenarios aren’t only related to the amount
of code one has to write. Bambi also prevents us from making mistakes when writing
the PyMC3 model, such as the mistakes I was making when specifying the shapes
for the distributions. Or one could just simply don’t know how correctly
prepare the data that should go in the design matrix, such as the conversion of the
categorical data to numeric matrices in such a way that the information is
retained without introducing structural redundancies.</p>
<p>Nevertheless, this doesn’t mean we should always favor Bambi over PyMC3.
Whether Bambi or PyMC3 is appropriate for you actually depends on your use case.
If you’re someone who mainly needs to fit GLMs or GLMMs, Bambi is the way to go
and it would be nice you give it a chance.
There are a <a href="https://bambinos.github.io/bambi/master/examples.html">bunch of examples</a>
showing how to specify and fit different GLMs with Bambi. On the other hand,
if you’re someone who writes a lot of custom models, PyMC3 will be your best
friend when it comes to working with Bayesian models in Python.</p>
<p>Bambi is a community project and welcomes contributions such as bug fixes,
examples, issues related to bugs or desired enhancements, etc.
Want to know more? Visit the <a href="https://bambinos.github.io/bambi/master/index.html">official docs</a>
or explore the <a href="https://github.com/bambinos/bambi">Github repo</a>. Also, if you
have any doubts about whether the feature you want is available or going to be
developed, feel free to reach out to us! You can always open a new issue to
request a feature or leave feedback about the library, and we welcome them a
lot 😁.</p>
</div>
<div id="acknowledgments" class="section level2">
<h2>Acknowledgments</h2>
<p>I want to thank <a href="https://twitter.com/AgustinaArroyu1">Agustina</a>,
<a href="https://twitter.com/canyon289">Ravin</a>, and <a href="https://twitter.com/aloctavodia">Osvaldo</a>
for very useful comments and feedback on an earlier version of this post. They
helped me to make this post much nicer than what it was originally.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Version 0.5.0<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>

</div>


  </main>

  <footer>
  <div class="copyright">
    &copy; Tomás Capretto 2021 · <a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>
  </div>
  
  <!DOCTYPE html>
  
  <html>
  <head>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    
    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
      
      
      <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
      onload="renderMathInElement(document.body);"></script>
        </head>
        ...
      </html>  
  </body>
</footer>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/js/all.min.js"
          integrity="sha256-MAgcygDRahs+F/Nk5Vz387whB4kSK9NXlDN3w58LLq0="
          crossorigin="anonymous"></script>

  

  
</body>
</html>
